{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "552c9f4f-2d34-4e8a-ab05-7adc16c964b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from utils import generate_data\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.base.model import GenericLikelihoodModel\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbdb0a23-deda-4aef-8c8e-1a013baf5b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "data = generate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7aa5b237-d566-4781-a50b-a2ab861348dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 6.042689\n",
      "         Iterations: 20\n",
      "         Function evaluations: 23\n",
      "         Gradient evaluations: 23\n",
      "                       Results: GeneralizedPoisson\n",
      "=========================================================================\n",
      "Model:                 GeneralizedPoisson  Pseudo R-squared:  0.046      \n",
      "Dependent Variable:    y                   AIC:               13257.4897 \n",
      "Date:                  2022-04-02 14:34    BIC:               13317.4718 \n",
      "No. Observations:      1095                Log-Likelihood:    -6616.7    \n",
      "Df Model:              10                  LL-Null:           -6938.4    \n",
      "Df Residuals:          1084                LLR p-value:       9.1750e-132\n",
      "Converged:             1.0000              Scale:             1.0000     \n",
      "-------------------------------------------------------------------------\n",
      "                          Coef.  Std.Err.    z     P>|z|   [0.025  0.975]\n",
      "-------------------------------------------------------------------------\n",
      "intercept                 5.2691   0.0390 135.0435 0.0000  5.1927  5.3456\n",
      "days_since_start          0.3443   0.0149  23.0411 0.0000  0.3150  0.3736\n",
      "days_since_start_squared -0.1218   0.0140  -8.7024 0.0000 -0.1493 -0.0944\n",
      "day_of_week_1             0.1972   0.0513   3.8433 0.0001  0.0966  0.2978\n",
      "day_of_week_2             0.1864   0.0517   3.6085 0.0003  0.0852  0.2876\n",
      "day_of_week_3             0.2741   0.0509   5.3840 0.0000  0.1743  0.3738\n",
      "day_of_week_4             0.2891   0.0509   5.6839 0.0000  0.1894  0.3887\n",
      "day_of_week_5             0.1664   0.0516   3.2239 0.0013  0.0652  0.2675\n",
      "day_of_week_6             0.0225   0.0527   0.4267 0.6696 -0.0809  0.1259\n",
      "seasonality_cos           0.1196   0.0190   6.2817 0.0000  0.0823  0.1569\n",
      "seasonality_sin          -0.0527   0.0199  -2.6504 0.0080 -0.0917 -0.0137\n",
      "alpha                     7.2742   0.2190  33.2124 0.0000  6.8449  7.7035\n",
      "=========================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpois_mdl = sm.GeneralizedPoisson(data.y, data.x_df)\n",
    "gpois_res = gpois_mdl.fit()\n",
    "print(gpois_res.summary2())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f575101f-9321-4dbb-87ca-480d3ddabfd6",
   "metadata": {},
   "source": [
    "## Extend Generic Likelihood Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a77353f-b154-45db-b10d-7135652f01ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _vec_matrix_multiply(a, B):\n",
    "    return np.apply_along_axis(lambda x: x * a, 0, B)\n",
    "\n",
    "def _ll_latentnorm(y, X, beta, alph):\n",
    "    mu = (np.dot(X, beta)) # Should we exponentiate this??\n",
    "    sigma = np.exp(np.dot(X, alph))\n",
    "    Phi_bar = stats.norm(mu, sigma).cdf(np.log1p(y))\n",
    "    Phi_underbar = stats.norm(mu, sigma).cdf(np.log(y))\n",
    "    ll = np.log(Phi_bar - Phi_underbar)\n",
    "    #print(Phi_bar.sum())\n",
    "    #print(Phi_underbar.sum())\n",
    "    #print(ll.sum())\n",
    "    return ll\n",
    "\n",
    "def _gradutils(y, X, beta, alph):\n",
    "    mu = (np.dot(X, beta)) # Should we exponentiate this??\n",
    "    sigma = np.exp(np.dot(X, alph))\n",
    "\n",
    "    z_bar = (np.log1p(y) - mu) / sigma \n",
    "    z_underbar = (np.log(y) - mu) / sigma\n",
    "\n",
    "    Phi_bar = stats.norm.cdf(z_bar)\n",
    "    Phi_underbar = stats.norm.cdf(z_underbar)\n",
    "    Phi  = Phi_bar - Phi_underbar\n",
    "\n",
    "    phi_bar = stats.norm.pdf(z_bar)\n",
    "    phi_underbar = stats.norm.pdf(z_underbar)\n",
    "    phi = phi_bar - phi_underbar\n",
    "    \n",
    "    kappa_0 = phi / Phi\n",
    "    kappa_1 = (z_bar * phi_bar - z_underbar * phi_underbar) / Phi\n",
    "    kappa_2 = (z_bar**2 * phi_bar - z_underbar**2 * phi_underbar) / Phi\n",
    "    kappa_3 = (z_bar**3 * phi_bar - z_underbar**3 * phi_underbar) / Phi\n",
    "    \n",
    "    return kappa_0, kappa_1, kappa_2, kappa_3, mu, sigma\n",
    "\n",
    "def _em_gradutils(W, sigma, c, alpha, return_hessian=False):\n",
    "    sigma_neg_2 = sigma**-2\n",
    "    grad = W.T @ (sigma_neg_2 * c - 1.) - alpha\n",
    "    hessian = None\n",
    "    if return_hessian:\n",
    "        W_sqrt_k = _vec_matrix_multiply(np.sqrt(c)/sigma, W)\n",
    "        hessian = -2. * W_sqrt_k.T @ W_sqrt_k\n",
    "    return grad, hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "610c86ae-73a3-44b8-8464-213ecdd8db55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLatentNormalEM(GenericLikelihoodModel):\n",
    "    def __init__(self, endog, exog, penalty=0., **kwds):\n",
    "        super(MyLatentNormalEM, self).__init__(endog, exog, **kwds)\n",
    "        self.nparams = 22\n",
    "        self.penalty = penalty\n",
    "        \n",
    "        \n",
    "    def nloglikeobs(self, params):\n",
    "        exog = self.exog\n",
    "        endog = self.endog\n",
    "        beta = params[:11] #first 11 are for mu\n",
    "        alph = params[11:] #last 11 are for sigma\n",
    "        ll = _ll_latentnorm(endog, exog, beta, alph)\n",
    "        params_alt = params.copy()\n",
    "        params_alt[0] = 0.\n",
    "        return -ll + self.penalty*np.sum(params_alt**2)/self.endog.size\n",
    "    \n",
    "    def score(self, params):\n",
    "        X = self.exog\n",
    "        y = self.endog\n",
    "        beta = params[:11] #first 11 are for mu\n",
    "        alph = params[11:] #last 11 are for sigma\n",
    "        \n",
    "        kappa_0, kappa_1, kappa_2, kappa_3, mu, sigma = _gradutils(y, X, beta, alph)\n",
    "        \n",
    "        beta_alt = beta.copy()\n",
    "        beta_alt[0] = 0\n",
    "        alph_alt = alph.copy()\n",
    "#         alph_alt[0] = 0.\n",
    "        \n",
    "        grad_beta = -(kappa_0 / sigma) @ X - self.penalty*2 * beta_alt\n",
    "        grad_alph = -kappa_1 @ X - self.penalty*2 * alph_alt\n",
    "        \n",
    "        return np.append(grad_beta, grad_alph)\n",
    "    \n",
    "    def hessian(self, params):\n",
    "        y = self.endog\n",
    "        X = self.exog\n",
    "        beta = params[:11] #first 11 are for mu\n",
    "        alph = params[11:] #last 11 are for sigma\n",
    "        \n",
    "        kappa_0, kappa_1, kappa_2, kappa_3, mu, sigma = _gradutils(y, X, beta, alph)\n",
    "        \n",
    "        k_beta = (kappa_0**2 + kappa_1) / sigma**2\n",
    "        k_alph = kappa_1 * (kappa_1 - 1) + kappa_3\n",
    "        k_beta_alph = (kappa_2 + kappa_0*(kappa_1 - 1)) / sigma\n",
    "        H_beta = np.zeros([11, 11])\n",
    "        H_alph = np.zeros([11, 11])\n",
    "        H_beta_alph = np.zeros([11, 11])\n",
    "                  \n",
    "        for i in range(X.shape[0]):\n",
    "            x = X[i]\n",
    "            xxT = np.outer(x, x)\n",
    "            H_beta -= k_beta[i] * xxT\n",
    "            H_alph -= k_alph[i] * xxT\n",
    "            H_beta_alph -= k_beta_alph[i] * xxT\n",
    "        \n",
    "        H_all = np.block([[H_beta, H_beta_alph], [H_beta_alph.T, H_alph]]) # 22 x 22\n",
    "        penalty_matrix = self.penalty*2 * np.eye(22)\n",
    "        penalty_matrix[0, 0] = 0.\n",
    "\n",
    "        return H_all - penalty_matrix\n",
    "        \n",
    "    \n",
    "    def fit(self, start_params=None, maxiter=100, use_hessian=False, step_size=1e-4, tol=1e-6):\n",
    "        start_time = time.time()\n",
    "        if start_params is None:\n",
    "            # Reasonable starting values\n",
    "            start_params = np.zeros(self.nparams)\n",
    "            start_params[0] = np.log(np.mean(self.endog)) # beta intercept\n",
    "        self.em(\n",
    "            start_params=start_params,\n",
    "            maxiter=maxiter,\n",
    "            use_hessian=use_hessian,\n",
    "            step_size=step_size,\n",
    "            tol=tol)\n",
    "        end_time = time.time()\n",
    "        self.wall_time = end_time - start_time\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def update_beta(self, beta):\n",
    "        self.beta = beta\n",
    "        self.mu = self.exog @ beta\n",
    "        \n",
    "        \n",
    "    def update_alpha(self, alpha):\n",
    "        self.alpha = alpha\n",
    "        self.sigma = np.exp(self.exog @ alpha)\n",
    "    \n",
    "    \n",
    "    def update_expectations(self):\n",
    "        kappa_0, kappa_1, kappa_2, kappa_3, mu, sigma = _gradutils(self.endog, self.exog, self.beta, self.alpha)\n",
    "        self.e1 = self.mu - self.sigma * kappa_0\n",
    "        sigma2 = self.sigma**2\n",
    "        self.e2 = (\n",
    "            sigma2 -\n",
    "            sigma2 * kappa_1 +\n",
    "            self.mu**2 -\n",
    "            2*self.mu*self.sigma*kappa_0\n",
    "        )\n",
    "        \n",
    "\n",
    "    def em(self, start_params, maxiter, use_hessian=False, step_size=1e-4, tol=1e-4):\n",
    "        # Starting values\n",
    "        loss = self.nloglikeobs(start_params).mean()\n",
    "        self.update_beta(start_params[:11])\n",
    "        self.update_alpha(start_params[11:])\n",
    "        X = self.exog\n",
    "        W = self.exog\n",
    "        penalty_alpha = self.penalty * np.eye(11)\n",
    "        WtW_plus_penalty = W.T @ W + penalty_alpha\n",
    "        penalty_beta = penalty_alpha.copy()\n",
    "        penalty_beta[0] = 0.\n",
    "        converged = False\n",
    "        \n",
    "        for i in range(maxiter):\n",
    "#             print(f\"Iteration {i} loss: {loss}\")\n",
    "            loss_last = loss.copy()\n",
    "            self.update_expectations()\n",
    "                                    \n",
    "            # Calculate beta\n",
    "            X_sqrt_w = _vec_matrix_multiply(1./self.sigma, X)\n",
    "            XtSiX = X_sqrt_w.T @ X_sqrt_w\n",
    "            XtSiX += penalty_beta\n",
    "            XtSie1 = X.T @ (self.sigma**-2 * self.e1)\n",
    "            beta = np.linalg.solve(XtSiX, XtSie1)\n",
    "            \n",
    "            # Calculate alpha\n",
    "            c = self.e2 - 2*self.e1*self.mu + self.mu**2 # NOTE: This is using the updated mu\n",
    "            if use_hessian:\n",
    "                grad, hessian = _em_gradutils(W, self.sigma, c, self.alpha, return_hessian=True)\n",
    "                grad -= self.alpha\n",
    "                hessian -= penalty_alpha\n",
    "                alpha = self.alpha - np.linalg.solve(hessian, grad)\n",
    "            else:\n",
    "                # Backtracking line search\n",
    "                grad, _ = _em_gradutils(W, self.sigma, c, self.alpha)\n",
    "                d = -grad # Descent direction\n",
    "                prop_increase = 0.5 # Called alpha in notes\n",
    "                step_multiplier = 0.5 # Called beta in notes\n",
    "                curr_step_size = step_size # Called eta in notes\n",
    "                f_start = self.nloglikeobs(np.concatenate([self.beta, self.alpha])).sum()\n",
    "                while True:\n",
    "                    alpha = self.alpha - curr_step_size * d\n",
    "                    f_stop = self.nloglikeobs(np.concatenate([self.beta, alpha])).sum()\n",
    "                    required_change = prop_increase * curr_step_size * (grad @ d)\n",
    "                    if f_stop - f_start <= required_change:\n",
    "                        break\n",
    "                    curr_step_size *= step_multiplier\n",
    "            \n",
    "            # Update alpha, beta simultaneously\n",
    "            self.update_alpha(alpha)\n",
    "            self.update_beta(beta)\n",
    "\n",
    "            # Check convergence\n",
    "            params = np.concatenate([self.beta, self.alpha])\n",
    "            loss = self.nloglikeobs(params).mean()\n",
    "            obj = loss_last - loss # Want this to be positive\n",
    "            if abs(obj) < tol: # Not enforcing any sort of sign constraint for now\n",
    "                converged = True\n",
    "                break\n",
    "        else:\n",
    "            raise RuntimeError(\"Hit maxiter and failed to converge\")\n",
    "            \n",
    "        self.params = np.concatenate([self.beta, self.alpha])\n",
    "        self.iters = i\n",
    "        self.loss = loss\n",
    "        self.loss_last = loss_last\n",
    "        self.obj = obj\n",
    "        self.converged = converged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6f973da-0a98-4140-8646-cb61670ac396",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = list(data.x_df)\n",
    "names_alpha = [s + \"_alpha\" for s in names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97fe20d4-dda8-4612-9b89-d2734d05e061",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = MyLatentNormalEM(data.y, data.x_df, penalty=1., extra_params_names=names_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c2486e0-d829-471b-b905-b2d1323d7fc3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.012996196746826172"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Second-order\n",
    "start_params = np.zeros(22)\n",
    "start_params[:11] = gpois_res.params[:11] # Warm start with estimates of betas\n",
    "start_params[11] = -1. # It's really sensitive to this starting value\n",
    "mod_res = mod.fit(start_params=start_params, maxiter=1000, use_hessian=True, tol=1e-6)\n",
    "mod_res.wall_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46c75a6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35121583938598633"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First-order\n",
    "start_params = np.zeros(22)\n",
    "start_params[:11] = gpois_res.params[:11] # Warm start with estimates of betas\n",
    "start_params[11] = -1. # It's really sensitive to this starting value\n",
    "mod_res = mod.fit(start_params=start_params, maxiter=1000, use_hessian=False, step_size=1e-3, tol=1e-6)\n",
    "mod_res.wall_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0eb650a0-0554-4ba4-b4f4-d2e1b8a53c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5.0362054   0.39143688 -0.15850647  0.27498884  0.28769868  0.38168434\n",
      "  0.43667161  0.24492697  0.04618936  0.17909704 -0.07685382]\n",
      "True Beta: [ 5.    0.4  -0.17  0.33  0.36  0.4   0.39  0.26  0.07  0.2  -0.1 ]\n",
      "[-1.05058743 -0.19967601 -0.05849583  0.25145074  0.42198571  0.45896794\n",
      "  0.54649341  0.24284324  0.09970537  0.1922257  -0.09557015]\n",
      "True Alpha: [-1.   -0.2  -0.03  0.33  0.36  0.4   0.39  0.26  0.07  0.16 -0.05]\n"
     ]
    }
   ],
   "source": [
    "print(mod_res.beta)\n",
    "print(f\"True Beta: {data.beta}\")\n",
    "print(mod_res.alpha)\n",
    "print(f\"True Alpha: {data.alpha}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a5bb5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
