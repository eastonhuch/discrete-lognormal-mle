{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ae44212",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import torch\n",
    "from torch.distributions.normal import Normal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b6e37c",
   "metadata": {},
   "source": [
    "# Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1da8d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: 0.056196969002485275\n",
      "alpha: 1.3264514207839966\n"
     ]
    }
   ],
   "source": [
    "# Gradient from Pytorch\n",
    "zero_tensor = torch.tensor(0.)\n",
    "one_tensor = torch.tensor(1.)\n",
    "w_torch = torch.normal(zero_tensor, one_tensor)\n",
    "x_torch = torch.normal(zero_tensor, one_tensor)\n",
    "y_torch = torch.tensor(1.)\n",
    "alpha_torch = torch.normal(zero_tensor, one_tensor)\n",
    "alpha_torch.requires_grad = True\n",
    "beta_torch = torch.normal(zero_tensor, one_tensor)\n",
    "beta_torch.requires_grad = True\n",
    "mu_torch = x_torch * beta_torch\n",
    "sigma_torch = (w_torch * alpha_torch).exp()\n",
    "normal_dist = Normal(loc=mu_torch, scale=sigma_torch)\n",
    "prob = normal_dist.cdf((y_torch+1).log()) - normal_dist.cdf(y_torch.log())\n",
    "log_prob = prob.log()\n",
    "log_prob.backward()\n",
    "print(f\"beta: {beta_torch.grad}\")\n",
    "print(f\"alpha: {alpha_torch.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ae583b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: 0.05613741642518377\n",
      "alpha: 1.3264086067601433\n"
     ]
    }
   ],
   "source": [
    "# Numerical derivative\n",
    "w = w_torch.detach().numpy()\n",
    "x = x_torch.detach().numpy()\n",
    "y = y_torch.detach().numpy()\n",
    "alpha = alpha_torch.detach().numpy()\n",
    "beta = beta_torch.detach().numpy()\n",
    "mu = mu_torch.detach().numpy()\n",
    "sigma = sigma_torch.detach().numpy()\n",
    "\n",
    "def log_pmf(y, mu, sigma):\n",
    "    Phi_right = norm.cdf(x=np.log(y+1), loc=mu, scale=sigma)\n",
    "    Phi_left = norm.cdf(x=np.log(y), loc=mu, scale=sigma)\n",
    "    prob = Phi_right - Phi_left\n",
    "    log_prob = np.log(prob)\n",
    "    return log_prob\n",
    "\n",
    "dbeta = 1e-3\n",
    "deriv_beta_numeric = (log_pmf(y, mu + x*dbeta, sigma) - log_pmf(y, mu, sigma)) / dbeta\n",
    "print(f\"beta: {deriv_beta_numeric}\")\n",
    "\n",
    "dalpha = 1e-3\n",
    "deriv_alpha_numeric = (log_pmf(y, mu, sigma * np.exp(w * dalpha)) - log_pmf(y, mu, sigma)) / dalpha\n",
    "print(f\"alpha: {deriv_alpha_numeric}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "313fc92d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: 0.05619716554683066\n",
      "alpha: 1.3264510479330736\n"
     ]
    }
   ],
   "source": [
    "# Analytical gradient\n",
    "z_bar = (np.log(y+1)-mu)/sigma\n",
    "z_underbar = (np.log(y)-mu)/sigma\n",
    "beta_num = norm.pdf(z_underbar) - norm.pdf(z_bar)\n",
    "beta_den = norm.cdf(z_bar) - norm.cdf(z_underbar)\n",
    "beta_grad = beta_num / beta_den * x / sigma\n",
    "print(f\"beta: {beta_grad}\")\n",
    "\n",
    "# Simple analytical gradient\n",
    "alpha_num = z_underbar * norm.pdf(z_underbar) - z_bar * norm.pdf(z_bar)\n",
    "alpha_den = norm.cdf(z_bar) - norm.cdf(z_underbar)\n",
    "alpha_grad = alpha_num / alpha_den * w\n",
    "print(f\"alpha: {alpha_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0709f2f",
   "metadata": {},
   "source": [
    "# Hessians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a4246f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient from Pytorch\n",
    "zero_tensor = torch.tensor(0.)\n",
    "one_tensor = torch.tensor(1.)\n",
    "w_torch = torch.normal(zero_tensor, one_tensor)\n",
    "x_torch = torch.normal(zero_tensor, one_tensor)\n",
    "y_torch = torch.tensor(1.)\n",
    "alpha_torch = torch.normal(zero_tensor, one_tensor)\n",
    "alpha_torch.requires_grad = True\n",
    "beta_torch = torch.normal(zero_tensor, one_tensor)\n",
    "beta_torch.requires_grad = True\n",
    "mu_torch = x_torch * beta_torch\n",
    "mu_torch.retain_grad()\n",
    "sigma_torch = (w_torch * alpha_torch).exp()\n",
    "sigma_torch.retain_grad()\n",
    "z_bar = ((y_torch+1).log() - mu_torch) / sigma_torch\n",
    "z_underbar = ((y_torch).log() - mu_torch) / sigma_torch\n",
    "normal_dist = Normal(loc=0, scale=1)\n",
    "Phi_bar = normal_dist.cdf(z_bar)\n",
    "Phi_underbar = normal_dist.cdf(z_underbar)\n",
    "pi = Phi_bar - Phi_underbar\n",
    "log_pi = pi.log()\n",
    "phi_bar = normal_dist.log_prob(z_bar).exp()\n",
    "phi_underbar = normal_dist.log_prob(z_underbar).exp()\n",
    "gamma = (phi_bar - phi_underbar) / pi\n",
    "delta = (z_bar * phi_bar - z_underbar * phi_underbar) / pi\n",
    "lambda_ = (z_bar**2 * phi_bar - z_underbar**2 * phi_underbar) / pi\n",
    "grad_beta = -gamma*x_torch/sigma_torch\n",
    "\n",
    "# Uncomment to check gradient of pi wrt mu\n",
    "# pi.backward()\n",
    "# print(mu_torch.grad)\n",
    "# print(-1/sigma_torch * (phi_bar - phi_underbar))\n",
    "\n",
    "# Uncomment to check gradient of log(pi) wrt beta\n",
    "# log_pi.backward()\n",
    "# print(f\"analytical: {grad_beta}\")\n",
    "# print(f\"torch: {beta_torch.grad}\")\n",
    "\n",
    "# Uncomment to check gradient of phi_bar - phi_underbar wrt mu\n",
    "# num = phi_bar - phi_underbar\n",
    "# num.backward()\n",
    "# print(mu_torch.grad)\n",
    "# print((z_bar * phi_bar - z_underbar * phi_underbar)/sigma_torch)\n",
    "# print(1/sigma_torch * gamma**2 - gamma)\n",
    "\n",
    "# Uncomment to check gradient of gamma wrt mu\n",
    "# gamma.backward()\n",
    "# print(mu_torch.grad)\n",
    "# print(1/sigma_torch * (delta + gamma**2))\n",
    "\n",
    "# Uncomment to check gradient of phi_bar wrt sigma\n",
    "# phi_bar.backward()\n",
    "# print(sigma_torch.grad)\n",
    "# print(phi_bar * z_bar**2 / sigma_torch)\n",
    "\n",
    "# Uncomment to check gradient of Phi_bar wrt sigma\n",
    "# Phi_bar.backward()\n",
    "# print(sigma_torch.grad)\n",
    "# print(-phi_bar * z_bar / sigma_torch)\n",
    "\n",
    "# Propagate gradient (comment if checking gradient)\n",
    "grad_beta.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "59519318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta^2 torch: -0.9477335214614868\n",
      "beta^2 analytical: -0.9477336406707764\n"
     ]
    }
   ],
   "source": [
    "print(f\"beta^2 torch: {beta_torch.grad}\")\n",
    "hessian_beta = -x_torch**2/sigma_torch**2 * (gamma**2 + delta)\n",
    "print(f\"beta^2 analytical: {hessian_beta}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "69c337a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta alpha torch: -1.0424295663833618\n",
      "beta alpha analytical: -1.0424295663833618\n"
     ]
    }
   ],
   "source": [
    "print(f\"beta alpha torch: {alpha_torch.grad}\")\n",
    "hessian_beta_alpha = -1/sigma_torch * x_torch * w_torch * (lambda_ + gamma * (delta - 1))\n",
    "print(f\"beta alpha analytical: {hessian_beta_alpha}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7c275352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.1520)\n",
      "tensor(-0.1520, grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Gradient from Pytorch\n",
    "zero_tensor = torch.tensor(0.)\n",
    "one_tensor = torch.tensor(1.)\n",
    "w_torch = torch.normal(zero_tensor, one_tensor)\n",
    "x_torch = torch.normal(zero_tensor, one_tensor)\n",
    "y_torch = torch.tensor(1.)\n",
    "alpha_torch = torch.normal(zero_tensor, one_tensor)\n",
    "alpha_torch.requires_grad = True\n",
    "beta_torch = torch.normal(zero_tensor, one_tensor)\n",
    "beta_torch.requires_grad = True\n",
    "mu_torch = x_torch * beta_torch\n",
    "mu_torch.retain_grad()\n",
    "sigma_torch = (w_torch * alpha_torch).exp()\n",
    "sigma_torch.retain_grad()\n",
    "z_bar = ((y_torch+1).log() - mu_torch) / sigma_torch\n",
    "z_underbar = ((y_torch).log() - mu_torch) / sigma_torch\n",
    "normal_dist = Normal(loc=0, scale=1)\n",
    "Phi_bar = normal_dist.cdf(z_bar)\n",
    "Phi_underbar = normal_dist.cdf(z_underbar)\n",
    "pi = Phi_bar - Phi_underbar\n",
    "log_pi = pi.log()\n",
    "phi_bar = normal_dist.log_prob(z_bar).exp()\n",
    "phi_underbar = normal_dist.log_prob(z_underbar).exp()\n",
    "gamma = (phi_bar - phi_underbar) / pi\n",
    "delta = (z_bar * phi_bar - z_underbar * phi_underbar) / pi\n",
    "lambda_ = (z_bar**2 * phi_bar - z_underbar**2 * phi_underbar) / pi\n",
    "kappa = (z_bar * (z_bar**2 - 1) * phi_bar - z_underbar*(z_underbar**2-1) * phi_underbar) / pi\n",
    "grad_alpha = -delta*w_torch\n",
    "\n",
    "# Uncomment to check gradient of z_bar * phi_bar wrt sigma\n",
    "# z_bar_phi_bar = z_bar * phi_bar\n",
    "# z_bar_phi_bar.backward()\n",
    "# print(sigma_torch.grad)\n",
    "# print(z_bar * phi_bar / sigma_torch * (z_bar**2 - 1))\n",
    "\n",
    "# Propagate gradient (comment if checking gradient)\n",
    "grad_alpha.backward()\n",
    "\n",
    "print(alpha_torch.grad)\n",
    "print(-(kappa + delta**2) * w_torch**2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
